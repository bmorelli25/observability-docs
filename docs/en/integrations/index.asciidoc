include::{docs-root}/shared/versions/stack/{source_branch}.asciidoc[]
include::{docs-root}/shared/attributes.asciidoc[]

:doctype: book
:forum: https://discuss.elastic.co/

:package-spec-repo: https://github.com/elastic/package-spec

// Access package spec files with {package-spec-root}
:package-spec-version: 1
:package-spec: {package-spec-root}/versions/{package-spec-version}

= Integrations Developer Guide

// Included file description
// include::file-name.asciidoc[leveloffset=+1]

// Build these docs
// $GIT_HOME/docs/build_docs --doc $GIT_HOME/observability-docs/docs/en/integrations/index.asciidoc --resource $GIT_HOME/package-spec/versions --chunk 1 --open

[[what-is-an-integration]]
== What is an integration?

// Ingest Observability data from popular services into the Elastic Stack
// https://github.com/elastic/integrations
// https://github.com/elastic/integrations/blob/master/docs/definitions.md
// This needs WORK

An Elastic integration is a collection of assets that defines how to observe a specific product with the Elastic Stack.
Integrations offer a number of benefits over other Observability options:

* Structured around the service that is being observed--not the monitoring agent
* Easy, less error-prone configuration
* Fewer monitoring agents for users to install
* Deploy in just a few clicks
* Decoupled release process from the Elastic Stack

The assets contained in an integration can define every step of the observability process:

* Data transformation, ingestion, and storage.
* Data visualization
* Elastic Agent configuration
* Documentation
* Tests

Integrations have a strict, well-defined structure.
This structure is described by the package specification.

[discrete]
[[why-build-an-integration]]
=== Why build an integration?

* You want to observe _some_ service
* You observe that service by ingesting metrics and/or logs from it
* You want to visualize this data in a meaningful way

[[integration-lifecycle]]
== Integration lifecycle

. Create a source package
+
All integrations start as a source package.
You'll find most Elastic integrations in the `elastic/integrations` repo,
but a package can live anywhere.

. Publish to the package registry
+
When an integration is ready to be published, bump the version in the manifest file
and publish the package to the Elastic Package Registry (EPR) using `elastic-package`
// the three stages of EPR reflect the maturity of a package
// snapshot
// staging
// production

. Install the package
+
After a package is published in the EPR, it is downloaded as a .zip file from the package-registry by Fleet inside Kibana.
The package is now ready to be installed.
+
Click **add integration** to install the integration and add it to an Elastic Agent policy.
When you install a package, its assets are unpacked and installed into Elasticsearch and Kibana using stack APIs.
In addition, configuration for the package is persisted in Elasticsearch as an Elastic Agent policy.

. Add the policy with the integration to an Elastic Agent
+
Once the policy with an integration is added to an Elastic Agent,
the agent will begin to collect and ship data to the Elastic Stack based on the Elastic integration.
+
Package assets may come into play here. For example, if a package installed ingest pipelines,
those will intercept the data, and transform it before it is indexed.

. Visualize
+
Integrations can ship with custom dashboards and visualizations that are installed with the integration.
Use these for a tailored view at your Observability data.

[discrete]
[[integration-repos]]
=== Repositories

// While this repository contains sources for Elastic Integrations, built Elastic Integrations are stored in the Package Storage repository and served up via the Package Registry. The Fleet UI in Kibana connects to the Package Registry and allows users to discover, install, and configure Elastic Packages.

[discrete]
==== Package specification

* Formal spec of what an Elastic package is
* What we use for validation of new or updated packages

Repo: https://github.com/elastic/package-spec

Right now it's on V1. Should we pull some of this content into the documentation?
https://github.com/elastic/package-spec/tree/master/versions/1

[discrete]
==== Package storage

* Backing store for the package registry service
* Whatever you see in this repo is what you can find via the package registry service
* 3 branches: snapshot, staging, production

Repo: https://github.com/elastic/package-storage

[discrete]
==== Package registry

* Source code for the package registry server

Repo: https://github.com/elastic/package-registry

[discrete]
==== Integrations

* Where most _Elastic_ packages are kept

Repo: https://github.com/elastic/integrations

[[specification]]
== Specification

Integrations are a type of package and therefore must adhere to the Elastic package specification.
The package specification describes:

* The folder structure of a package and the expected files within these folders
* The structure of expected files' contents

[discrete]
[[asset-organization]]
=== Asset organization

In general, assets within a package are organized by `<elastic-stack-component>/<asset-type>`.
For example, ingest pipelines are stored in the `elasticsearch/ingest-pipeline` folder.
This logic applies to all Elasticsearch, Kibana, and Agent assets.

Top-level assets are picked up as JSON documents, and pushed to the corresponding Elasticsearch and Kibana APIs.

[discrete]
[[data-streams]]
==== Data streams

There is a special folder called `data_stream`.
Each data stream should have its own folder of assets within this folder,
and the names of these data streams must follow the data stream naming scheme.

The contents of these folders follow the `<elastic-stack-component>/<asset-type>` structure.
During installation, Fleet enforces data stream naming rules.
All assets in this folder belong directly or indirectly to data streams.

In most scenarios, only data stream assets are needed. There are exceptions where global assets are required to get more flexibility. This could be, for example, an ILM policy that applies to all data streams.

[discrete]
[[supported-assets]]
=== Supported assets

The following assets are typically found in an Elastic package:

* Elasticsearch
** Ingest Pipeline
** Index Template
** Transform
** Index template settings
* Kibana
** Dashboards
** Visualization
** Index patterns
** ML Modules
** Map
** Search
** Security rules
* Other
** fields.yml

[discrete]
[[directory-structure]]
=== Directory structure

[source,text]
----
apache
│   changelog.yml
│   manifest.yml
└───_dev
└───data_stream
└───docs
└───img
└───kibana
----

[discrete]
[[directory-spec]]
=== Spec

Included from the package-spec repository. This will update when the spec is updated.

[source,yaml]
----
include::{package-spec}/spec.yml[]
----

[[dev-spec]]
=== `_dev`

Development resources.

**required**

Included from the package-spec repository. This will update when the spec is updated.

[source,yaml]
----
include::{package-spec}/_dev/spec.yml[]
----

[[data-stream-spec]]
=== `data_stream`

Data stream assets, including ingest pipelines, field definitions, metadata, and sample events.

**required**

Included from the package-spec repository. This will update when the spec is updated.

[source,yaml]
----
include::{package-spec}/data_stream/spec.yml[]
----

[[docs-spec]]
=== `docs`

The built integration readme file.

**required**

Included from the package-spec repository. This will update when the spec is updated.

[source,yaml]
----
include::{package-spec}/docs/spec.yml[]
----

[[kibana-spec]]
=== `kibana`

The integration's Kibana assets, like dashboards, visualizations, machine learning modules, etc.

**required**

Included from the package-spec repository. This will update when the spec is updated.

[source,yaml]
----
include::{package-spec}/kibana/spec.yml[]
----

[[changelog-spec]]
=== `changelog.yml`

The integration's changelog.

**required**

Included from the package-spec repository. This will update when the spec is updated.

[source,yaml]
----
include::{package-spec}/changelog.spec.yml[]
----

[[manifest-spec]]
=== `manifest.yml`

Integration metadata, like version, name, license level, description, category,
icon and screenshot mappings, and policy template definitions.

**required**

Included from the package-spec repository. This will update when the spec is updated.

[source,yaml]
----
include::{package-spec}/manifest.spec.yml[]
----

[[integration-example-apache]]
== Integration example: Apache

Let's look at an example:
The https://github.com/elastic/integrations/tree/master/packages/apache[Apache integration].
This integration periodically:

* fetches **status metrics** from the Apache server
* fetches and parses **access logs** created by the Apache server
* fetches and parses **error logs** created by the Apache server

Each of these different data types is considered a **data stream**.

[discrete]
[[apache-data-streams]]
=== Data streams

****
**Data streams** allow you to store time series data across multiple indices while giving you a single named resource for requests.

[%collapsible]
.Expand to learn more
====
stuff
====
****

Each data stream is represented by a folder in the `data_stream` directory:

[source,text]
----
apache
└───data_stream
│   └───access <1>
│   └───error <1>
│   └───status <1>
----
<1> Apache's three different data streams

Let's choose one of these data streams to dig into: access logs.

[discrete]
[[apache-ingest-pipelines]]
=== Ingest pipelines

Access logs (or any logs) should be parsed into structured data prior to ingesting them into Elasticsearch.
To do this, integrations use **ingest pipelines**

****
**Ingest pipelines** let you perform common transformations on your data before indexing. For example, you can use pipelines to remove fields, extract values from text, and enrich your data.

A pipeline consists of a series of configurable tasks called processors. Each processor runs sequentially, making specific changes to incoming documents. After the processors have run, Elasticsearch adds the transformed documents to your data stream or index.

[%collapsible]
.Expand to learn more
====
stuff
====
****

Ingest pipelines are defined in the `elasticsearch/ingest_pipeline` directory.
They only apply to the parent data stream:

[source,text]
----
apache
└───data_stream
│   └───access
│   │   └───elasticsearch/ingest_pipeline
│   │          default.yml <1>
│   └───error
│   └───status
----
<1> The ingest pipeline definition for the access logs data stream of the apache integration

The ingest pipeline definition requires a description and an array of processors.
Here's a snippet of the access logs ingest pipeline:

[source,yaml]
----
description: "Pipeline for parsing Apache HTTP Server access logs."
processors:
- set:
    field: event.ingested
    value: '{{_ingest.timestamp}}'
- rename:
    field: message
    target_field: event.original
- remove:
    field: apache.access.time
    ignore_failure: true
----

For more information, see ((add a link here of some kind))

[discrete]
[[apache-mappings]]
=== Mappings

Ingest pipelines create fields in an Elasticsearch index, but don't define the fields themselves.
Each field needs a defined data type, or mapping.

****
**Mapping** is the process of defining how a document, and the fields it contains, are stored and indexed.
Each document is a collection of fields, which each have their own data type. When mapping your data, you create a mapping definition, which contains a list of fields that are pertinent to the document. A mapping definition also includes metadata fields, like the _source field, which customize how a document’s associated metadata is handled.

[%collapsible]
.Expand to learn more
====
stuff
====
****

Mappings are defined in the `fields` directory.
Like ingest pipelines, mappings only apply to the parent datastream.
The apache integration has four different field definitions:

[source,text]
----
apache
└───data_stream
│   └───access
│   │   └───elasticsearch/ingest_pipeline
│   │   │      default.yml
│   │   └───fields
│   │          agent.yml <1>
│   │          base-fields.yml <2>
│   │          ecs.yml <3>
│   │          fields.yml <4>
│   └───error
│   └───status
----
<1> ??
<2> `base-fields.yml` never changes and is required for all integrations
<3> Defines the relevant ECS fields
<4> Custom apache access log fields ??

Maybe include one here or something

[discrete]
[[apache-ecs]]
=== ECS fields

****
**ECS**

Something about ECS here.

[%collapsible]
.Expand to learn more
====
stuff
====
****

[discrete]
[[apache-policy-templates]]
=== Policy templates?

??

[discrete]
[[apache-kibana-assets]]
=== Kibana assets

Something about Kibana dashboards or visualizations

[discrete]
[[apache-everything-else]]
=== Everything else

Something about docs, screenshots, and tests

[[elastic-package]]
== `elastic-package`

`elastic-package` is a command line tool, written in Go, used for developing Elastic packages.
It can help you lint, format, test, build, and promote your packages.

// Currently, elastic-package only supports packages of type Elastic Integrations.

[discrete]
[[elastic-package-start]]
=== Get started

. Download and build the latest master of elastic-package binary:
+
[source,terminal]
----
git clone https://github.com/elastic/elastic-package.git
make build
----
+
TIP: Make sure that you've correctly set up the https://golang.org/doc/gopath_code.html#GOPATH[`$GOPATH` and `$PATH`]
environment variables. `elastic-package` must be accessible from your `$PATH`.

. Change into the directory of the package under development:
+
[source,terminal]
----
cd my-package
----

. Run the `help` command to see available commands
+
[source,terminal]
----
elastic-package help
----

[discrete]
[[elastic-package-command-reference]]
=== Command reference

The following `elastic-package` commands are available.
For more details on a specific command, run `elastic-package help <command>`.

Some commands have a _global context_, meaning that they can be executed from anywhere.
Other commands have a _package context_; these must be executed from somewhere under a package's
root folder, and the command will only operate on the contents of that package.

// *************************
// The following is copied directly from
// https://github.com/elastic/elastic-package/blob/master/README.md
// *************************

[discrete]
==== `elastic-package help`

_Context: global_

Use this command to get a listing of all commands available under `elastic-package` and a brief
description of what each command does.

[discrete]
==== `elastic-package build`

_Context: package_

Use this command to build a package. Currently it supports only the "integration" package type.

Built packages are stored in the "build/" folder located at the root folder of the local Git repository checkout that contains your package folder. The command will also render the README file in your package folder if there is a corresponding template file present in "_dev/build/docs/README.md". All "_dev" directories under your package will be omitted.

Built packages are served up by the Elastic Package Registry running locally (see "elastic-package stack"). If you want a local package to be served up by the local Elastic Package Registry, make sure to build that package first using "elastic-package build".

Built packages can also be published to the global package registry service.

[discrete]
==== `elastic-package check`

_Context: package_

Use this command to verify if the package is correct in terms of formatting, validation and building.

It will execute the format, lint, and build commands all at once, in that order.

[discrete]
==== `elastic-package clean`

_Context: package_

Use this command to clean resources used for building the package.

The command will remove built package files (in build/), files needed for managing the development stack (in ~/.elastic-package/stack/development) and stack service logs (in ~/.elastic-package/tmp/service_logs).

[discrete]
==== `elastic-package create`

_Context: global_

Use this command to create a new package or add more data streams.

The command can help bootstrap the first draft of a package using embedded package template. It can be used to extend the package with more data streams.

For details on how to create a new package, review the [HOWTO guide](https://github.com/elastic/elastic-package/blob/master/docs/howto/create_new_package.md).

[discrete]
==== `elastic-package export`

_Context: package_

Use this command to export assets relevant for the package, e.g. Kibana dashboards.

[discrete]
==== `elastic-package format`

_Context: package_

Use this command to format the package files.

The formatter supports JSON and YAML format, and skips "ingest_pipeline" directories as it's hard to correctly format Handlebars template files. Formatted files are being overwritten.

[discrete]
==== `elastic-package install`

_Context: package_

Use this command to install the package in Kibana.

The command uses Kibana API to install the package in Kibana. The package must be exposed via the Package Registry.

[discrete]
==== `elastic-package lint`

_Context: package_

Use this command to validate the contents of a package using the package specification (see: https://github.com/elastic/package-spec).

The command ensures that the package is aligned with the package spec and the README file is up-to-date with its template (if present).

[discrete]
==== `elastic-package profiles`

_Context: global_

Use this command to add, remove, and manage multiple config profiles.

Individual user profiles appear in ~/.elastic-package/stack, and contain all the config files needed by the "stack" subcommand.
Once a new profile is created, it can be specified with the -p flag, or the ELASTIC_PACKAGE_PROFILE environment variable.
User profiles are not overwritten on upgrade of elastic-stack, and can be freely modified to allow for different stack configs.

[discrete]
==== `elastic-package promote`

_Context: global_

Use this command to move packages between the snapshot, staging, and production stages of the package registry.

This command is intended primarily for use by administrators.

It allows for selecting packages for promotion and opens new pull requests to review changes. Please be aware that the tool checks out an in-memory Git repository and switches over branches (snapshot, staging and production), so it may take longer to promote a larger number of packages.

[discrete]
==== `elastic-package publish`

_Context: package_

Use this command to publish a new package revision.

The command checks if the package hasn't been already published (whether it's present in snapshot/staging/production branch or open as pull request). If the package revision hasn't been published, it will open a new pull request.

[discrete]
==== `elastic-package service`

_Context: package_

Use this command to boot up the service stack that can be observed with the package.

The command manages lifecycle of the service stack defined for the package ("_dev/deploy") for package development and testing purposes.

[discrete]
==== `elastic-package stack`

_Context: global_

Use this command to spin up a Docker-based Elastic Stack consisting of Elasticsearch, Kibana, and the Package Registry. By default the latest released version of the stack is spun up but it is possible to specify a different version, including SNAPSHOT versions.

For details on how to connect the service with the Elastic stack, see the [service command](https://github.com/elastic/elastic-package/blob/master/README.md#elastic-package-service).

[discrete]
==== `elastic-package status [package]`

_Context: package_

Use this command to display the current deployment status of a package.

If a package name is specified, then information about that package is
returned, otherwise this command checks if the current directory is a
package directory and reports its status.

[discrete]
==== `elastic-package test`

_Context: package_

Use this command to run tests on a package. Currently, the following types of tests are available:

[discrete]
===== Asset Loading Tests
These tests ensure that all the Elasticsearch and Kibana assets defined by your package get loaded up as expected.

For details on how to run asset loading tests for a package, see the [HOWTO guide](https://github.com/elastic/elastic-package/blob/master/docs/howto/asset_testing.md).

[discrete]
===== Pipeline Tests
These tests allow you to exercise any Ingest Node Pipelines defined by your packages.

For details on how to configure pipeline test for a package, review the [HOWTO guide](https://github.com/elastic/elastic-package/blob/master/docs/howto/pipeline_testing.md).

[discrete]
===== Static Tests
These tests allow you to verify if all static resources of the package are valid, e.g. if all fields of the sample_event.json are documented.

For details on how to run static tests for a package, see the [HOWTO guide](https://github.com/elastic/elastic-package/blob/master/docs/howto/static_testing.md).

[discrete]
===== System Tests
These tests allow you to test a package's ability to ingest data end-to-end.

For details on how to configure amd run system tests, review the [HOWTO guide](https://github.com/elastic/elastic-package/blob/master/docs/howto/system_testing.md).

[discrete]
==== `elastic-package uninstall`

_Context: package_

Use this command to uninstall the package in Kibana.

The command uses Kibana API to uninstall the package in Kibana. The package must be exposed via the Package Registry.

[discrete]
==== `elastic-package version`

_Context: global_

Use this command to print the version of elastic-package that you have installed. This is especially useful when reporting bugs.

// *************************
// End COPIED CONTENT
// *************************

[[build-a-new-integration]]
== Build a new integration

You have something you want to monitor and you're ready to build an integration to do it.
Uh... Now what?

[[build-integration-prerequisites]]
=== Prerequisites

Before building an integration, you should have an understanding of the following concepts:

* the <<integration-lifecycle>>
* the package <<specification>>
* Elastic Stack concepts, like data streams, ingest pipelines, and mappings

TIP: See <<integration-example-apache>> for a breakdown of a real integration.

In addition, you must have <<elastic-package>> installed on your machine.

=== Spin up the Elastic Stack

Bring up the Elastic Stack with `elastic-package`:

[source,terminal]
----
elastic-package stack up -v -d
----

=== Create a new package

An easy way to create a new package is by copying a preexisting one.
This command copies the contents of the `nginx` package to a directory named `new_package`:

[source,terminal]
----
cd packages
cp -r nginx new_package
----

=== Review assets

The fun begins.
Remove unnecessary assets, adjust manifests, and create new data streams.

**We need more here. A lot more.**

=== Build

Next, build the package:

[source,terminal]
----
elastic-package build
----

Run the following command from inside of the integration directory to recycle the package-registry docker container.
This refreshes the Fleet UI, allowing it to pick up the new integration in Kibana.

[source,terminal]
----
elastic-package stack up --services package-registry
----

=== Lint

Verify the package is aligned with the specification with the lint command:

[source,terminal]
----
elastic-package lint
----

Problems and potential solutions will be shown.
Fix them and rebuild the package.

=== Format

Format the package contents (JSON and YAML files) with the format command:

[source,terminal]
----
elastic-package format
----

=== Test

`elastic-package` supports multiple types of tests -- pipeline, system, assets, and more.

See LINK_TO_TESTING_DOCS for more information.

// https://github.com/elastic/elastic-package/tree/master/docs/howto
// https://github.com/elastic/integrations/blob/master/docs/testing_and_validation.md

=== Final checklist

// https://github.com/elastic/integrations/blob/master/docs/fine_tune_integration.md

==== Add an icon

The integration icons are presented in different places in Kibana, hence it's better to define custom icons to make the UI easier to navigate.

==== Add screenshots

The Kibana Integration Manager shows screenshots related with the integration. Screenshots present Kibana dashboards visualizing the metric/log data.

==== Create a readme file

The README template is used to render the final README file including exported fields. The template should be placed in the package/<integration-name>/_dev/build/docs/README.md. If the directory doesn't exist, please create it.

Review the MySQL docs template to see how to use template functions (e.g. {{fields "data-stream-name"}}). If the same data stream name is used in both metrics and logs, please add -metrics and -logs in the template. For example, elb is a data stream for log and also a data stream for metrics. In README.md template, {{fields "elb_logs"}} and {{fields "elb_metrics"}} are used to separate them.

==== Review artifacts

==== Define variable properties

The variable properties customize visualization of configuration options in the Kibana UI. Make sure they're defined in all manifest files.

[source,yaml]
----
vars:
  - name: paths
    required: true <1>
    show_user: true <2>
    title: Access log paths <3>
    description: Paths to the nginx access log file. <4>
    type: text <5>
    multi: true <6>
    default:
      - /var/log/nginx/access.log*
----
<1> option is required
<2> don't hide the configuration option (collapsed menu)
<3> human readable variable name
<4> variable description (may contain some details)
<5> field type (according to the reference: text, password, bool, integer)
<6> the field has mutliple values.

==== Add sample events

=== Promote (?)

// https://github.com/elastic/integrations/blob/master/docs/developer_workflow_promote_release_integration.md

=== Open a PR

If you think that you've finished works on your integration, you've verified that it collects data, wrote some tests, you can open a PR to include your integration in the Integrations repository. The CI will verify if your integration is correct (elastic-package check) - a green status is a must.

// When the PR is merged, the CI will kick off a build job for the master branch, which can release your integration to the package-storage. It means that it will open a PR to the Package Storage/snapshot with the built integration if only the package version doesn't already exist in the storage (hasn't been released yet).

// When you are ready for your changes in the integration to be released, remember to bump up the package version. It is up to you, as the package developer, to decide how many changes you want to release in a single version. For example, you could implement a change in a PR and bump up the package version in the same PR. Or you could implement several changes across multiple PRs and then bump up the package version in the last of these PRs or in a separate follow up PR.

== Dashboards

sooooooon

// Not sure if this should be a separate section or not
// Kibana dashboards can be exported to local directories
[source,terminal]
----
elastic-package export
----

== Testing and validation

https://github.com/elastic/elastic-package/tree/master/docs/howto

=== Asset testing

https://github.com/elastic/elastic-package/tree/master/docs/howto

=== Pipeline testing

https://github.com/elastic/elastic-package/tree/master/docs/howto

=== Static testing

https://github.com/elastic/elastic-package/tree/master/docs/howto

=== System testing

https://github.com/elastic/elastic-package/tree/master/docs/howto

== Pull request review guidelines

soooooon

== Tutorial: Hello world integration

felt cute, might remove later

/end
